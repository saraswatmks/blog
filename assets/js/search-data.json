{
  
    
        "post0": {
            "title": "Packaging a Machine Learning Project using Python PEX",
            "content": "Introduction . Companies these days are looking for data scientists who can not only build machine learning models but also deploy them. Why? Because, companies are progressing towards data driven product/features development. Deploying models is not that easy. Specially, if you are a fresher or a data scientist who has been mainly doing dashboarding / data analysis. . In this tutorial, I will share my knowledge on the first step of deploying a machine learning model i.e. packing your model code. Here we’ll create a dummy machine learning project and learn to package it. . PEX (Python executable) provides an easy way to put together all the code &amp; dependencies and run it as an executable. You no longer have to worry about installing dependencies separately. . Note: This tutorial assumes no prior knowledge of how to package a python project. . Table of Contents . Basic Concepts | Writing Code | Build, Test and Package the Code | Deploy | 1. Basic Concepts . Before we start structuring the project, let’s get familiar with some basic concepts in programming: . Object Oriented Programming (OOP): In code packaging, using OOP style (classes &amp; objects) is a widely used methodolgy. Because, it helps in maintaining the code and helps in reducing possible bugs that would get overlooked otherwise. . Module: A module is directory (a folder) living in your project which can be imported. A simple way to identify a module is to look for __init__.py file. If this file exists in any directory, that directory will be a module. . Script:: A script is simply a .py code file which can be executed. A simple way to identify a script is to look for the entrypoint i.e. __name__ == &#39;__main__&#39; line. . Let’s start working now. . First things first, go to any directory, inside it, create a new directory named datapackage. Fill it files shown below: . └── datapack ├── config ├── datapack │   ├── __init__.py │   ├── docs │   └── utils ├── requirements.txt └── setup.py . Inside datapack, we will have the following structure: . config: is a directory which will contain the configuration files. | datapack: is a module which will contain the code. It’s a good practice to name it same as the project. docs: contains files/data to be used by the modules. | utils also contains code but mainly helper functions | . | requirements.txt contains a list of dependencies to be used. | setup.py this file will bundle our project into a package | This is the minimal structure one would need to create a python project. You can always add more directories to organise your project in a way that it is easier to maintain. . 2. Writing Code . Now, let’s add the code to our datapack project. Basically, it would do the following: . Read data from .csv file | Do preprocessing (text column mainly) | Train a model | Finally, return the predictions. | . Now, keep copy-pasting the code and create these scripts at your end. The code is simple and self-explanatory. . datapack/datapack/utils/preprocess.py : This script contains code to clean text data. We know this beforehand, because we know that our data contains a column called as text. . import re class PreProcess: @staticmethod def clean_data(data, stopwords): data[&#39;text&#39;] = data[&#39;text&#39;].apply(lambda x: re.sub(r&#39; W+&#39;,&#39; &#39;, x)) data[&#39;text&#39;] = data[&#39;text&#39;].str.lower() data[&#39;text&#39;] = data[&#39;text&#39;].apply(lambda x: &#39; &#39;.join([y for y in x.split() if y not in stopwords])) return data . datapack/datapack/main.py : This will be the entrypoint of the our project. . from datapack.utils.preprocess import PreProcess import pandas as pd import fir import configparser from sklearn.feature_extraction.text import CountVectorizer from sklearn.naive_bayes import GaussianNB class TrainPack: def __init__(self, config_file): self.config = configparser.ConfigParser() self.config.read(config_file) self.data = None self.stopwords = None def read_data(self): self.data = pd.read_csv(self.config.get(&#39;docs&#39;, &#39;data&#39;)) self.stopwords = pd.read_csv(self.config.get(&#39;docs&#39;, &#39;stopwords&#39;), header=None)[0].tolist() def clean_data(self): self.data = PreProcess().clean_data(data=self.data, stopwords=self.stopwords) def make_features(self): text_matrix = CountVectorizer(min_df=2).fit_transform(self.data[&#39;text&#39;]) return text_matrix.todense() def train_model(self): text_matrix = self.make_features() gnb = GaussianNB() pred = gnb.fit(text_matrix, self.data[&#39;label&#39;]).predict(text_matrix) return pred def run(self): self.read_data() self.clean_data() self.train_model() print(&#39;finished successfully&#39;) def compute(config_file): TrainPack(config_file=config_file).run() if __name__ == &#39;__main__&#39;: fire.Fire(compute) . datapack/setup.py : This file is super important for the successful packaging of the project. It defines the entire setup of the project i.e. which directory are the modules, where the data lives and creates .whl, .tar.gz extensions for package distribution. . from codecs import open as codecs_open from setuptools import setup, find_packages # Get the long description from the relevant file with codecs_open(&#39;README.md&#39;, encoding=&#39;utf-8&#39;) as f: long_description = f.read() setup( name=&#39;datapack&#39;, version=&#39;0.0.1&#39;, packages=find_packages(), package_data={&#39;datapack&#39;: [&#39;docs&#39;]}, url=&#39;&#39;, license=&#39;&#39;, description=&#39;Machine learning model&#39;, long_description=long_description ) . datapack/datapack/utils/__init__.py : This file should be empty. . datapack/datapack/__init__.py : Again, this file should be empty. . datapack/datapack/docs : contains the sample data. You can download the files from heredatapack/datapack/docs : contains the sample data. You can download the files from here: sample.csv and stopwords.txt . If you have followed all the steps as above, your project should look like this: . datapack ├── Makefile ├── README.md ├── config │   └── config.ini ├── datapack │   ├── __init__.py │   ├── docs │   │   ├── sample.csv │   │   └── stopwords.txt │   ├── main.py │   └── utils │   ├── __init__.py │   └── preprocess.py ├── requirements.txt └── setup.py . Last but not the least, we’ll create a Makefile which contains commandline code, which we might need to run many times. . datapack/Makefile . PYTHON=$(shell which python) ################################################################################# # Clean # ################################################################################# clean: rm -rf .pytest_cache rm -rf datapack.egg-info rm -rf target ################################################################################# # Build pex # ################################################################################# build: clean mkdir -p target pex . -v --disable-cache -r requirements.txt -R datapack/docs -o target/datapack.pex --python=$(PYTHON) ################################################################################# # Run pex # ################################################################################# run: $(PYTHON) target/datapack.pex -m datapack.main --config_file config/config.ini . Basically, it does the following: . clean: Remove all the unnessary residue files which will be created during building the pex. | build: Build the .pex files | run: Runs the pex as a standalone python executable | . 3. Build, Test and Package the code . That’s all the code we would need for this project, we are almost in our final stage. To package the project, go the package directory from the terminal and simply run: . make build . This would generate some messages on the terminal and finally your project would looks like: . datapack ├── Makefile ├── README.md ├── config │   └── config.ini ├── datapack │   ├── __init__.py │   ├── __pycache__ │   │   ├── __init__.cpython-36.pyc │   │   └── __init__.cpython-37.pyc │   ├── docs │   │   ├── sample.csv │   │   └── stopwords.txt │   ├── main.py │   └── utils │   ├── __init__.py │   ├── __pycache__ │   │   └── preprocess.cpython-37.pyc │   └── preprocess.py ├── datapack.egg-info │   ├── PKG-INFO │   ├── SOURCES.txt │   ├── dependency_links.txt │   └── top_level.txt ├── requirements.txt ├── setup.py └── target └── datapack.pex . Congrats, you’ve learned to package your code along with dependencies. target/datapack.pex is the packaged code. . Now, let’s test if it works. We do: . make run . It it works, on your terminal you should see : finished successfully . 4. Deploy . In deployment, the most complicated part is to manage the project dependencies. Many a times, your deployment would fail because of mismatch between dependencies in your development environment vs production environment. Trust me, it gets frustrating. You need to make sure that the machine/instance/pod on which you are deploying the model has the same dependencies version. But, no more with pex. . In our case, we have the dependencies along with the code. We just need to make sure about two things: . Python Version: You need to have the same python version installed to avoid conflicts. | Platform Dependency: .pex files are platform dependent. That means, you cannot run a .pex file in linux/windows which is build on mac. | If you’ve made sure the above two things, just simple copy the .pex file, config file your local to the deployment instance and run the actual make run command, which is: . python3 target/datapack.pex -m datapack.main --config_file config.ini . Summary . In this tutorial, we learned about packaging your python project using .pex library for deployment. We saw how to modularise the code using classes and objects. As mentioned above, pex files are nothing but standalone python interpreters. You could actually run .pex files as interpreter on your terminal. In the next tutorial, we’ll see how use python 3’s zipapp module to package the code. .",
            "url": "https://saraswatmks.github.io/blog/python/machinelearning/2020/06/19/packaging-machine-learning-project-python-pex.html",
            "relUrl": "/python/machinelearning/2020/06/19/packaging-machine-learning-project-python-pex.html",
            "date": " • Jun 19, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "How to create TF-IDF matrix using ngrams in R?",
            "content": "Introduction . Term Document/Inverse Document Frequency(TF-IDF) is a powerful text analysis technique to find similar documents based their vector representations. In simple words, it weights each token, not only on how many times it has occured in a particular document, but also across all set of documents (also called as corpus), to ensure that we put a lower weight to a token if it occurs too frequently (like stopwords). . Let’s see how we can create tf-idf matrix using ngrams. Later, we’ll train a simple random forest model on features generated from tf-idf. . You can install the package by doing: . install.packages(&quot;superml&quot;, dependencies = TRUE) . TF-IDF Matrix with superml . Superml following a scikit-learn style api, so if you are familiar with it, superml should be easy for you. In case you are new to it, just follow the explanation below. Superml is based on C++ optimised functions, hence it should be quite fast as well. . First, we’ll try to get a dummy dataset. . # download data, takes a few seconds data &lt;- readLines(&quot;https://www.r-bloggers.com/wp-content/uploads/2016/01/vent.txt&quot;) df &lt;- data.frame(data) colnames(df) &lt;- &#39;text&#39; # also let&#39;s add a target variable, so later we can train a model on tfidf features df$target &lt;- sample(c(0,1), size = nrow(df), replace = T) . Now, we create tfidf features. . library(superml) . ## Loading required package: R6 . tf &lt;- TfIdfVectorizer$new(ngram_range = c(1,3), max_df = 0.7) tf_feats &lt;- tf$fit_transform(df$text) . Explanation . ngram_range as c(1,3) means 1 is the minimum length of ngram and 3 is the maximum length of ngram. | max_df = 0.7 means ignore token (or word) should appear in more than 40% of the documents. | $new() method initialises the instance of TfIdfVectorizer class. | $fit_transform() return a matrix of tfidf features. | . Let’s see how the matrix looks like: . dim(tf_feats) . ## [1] 83 2059 . We get 2059 features for the given data. Let’s look at the tokens. . colnames(tf_feats)[1:30] . ## [1] &quot;s&quot; &quot;t&quot; &quot;charleston&quot; &quot;west&quot; ## [5] &quot;don&quot; &quot;people&quot; &quot;don t&quot; &quot;see&quot; ## [9] &quot;virginia&quot; &quot;west virginia&quot; &quot;just&quot; &quot;news&quot; ## [13] &quot;please&quot; &quot; liar&quot; &quot; s&quot; &quot;liar&quot; ## [17] &quot;obama&quot; &quot;republicans&quot; &quot;want&quot; &quot;can&quot; ## [21] &quot;good&quot; &quot;hillary&quot; &quot;problem&quot; &quot;re&quot; ## [25] &quot;show&quot; &quot;trump&quot; &quot;us&quot; &quot; liar &quot; ## [29] &quot;breaking&quot; &quot;breaking news&quot; . We see some text processing would be great before passing calculating the tfidf features. Let’s tke a look at the matrix. . head(tf_feats[1:3,50:60]) . ## problem anyone ben ben carson c carson clear deal drive every ## [1,] 0 0.000000 0 0 0 0 0 0.000000 0 0 ## [2,] 0 0.166419 0 0 0 0 0 0.166419 0 0 ## [3,] 0 0.000000 0 0 0 0 0 0.000000 0 0 ## iran ## [1,] 0.000000 ## [2,] 0.166419 ## [3,] 0.000000 . Now, let’s train a random forest model on these features. But, before that let’s split the data into train and test. . n_rows &lt;- nrow(df) %/% 2 train &lt;- data.frame(tf_feats[1:n_rows,]) train$target_var &lt;- df[1:n_rows,]$target test &lt;- data.frame(tf_feats[n_rows:nrow(df),]) test$target_var &lt;- df[n_rows:nrow(df),&#39;target&#39;] dim(train) . ## [1] 41 2060 . dim(test) . ## [1] 43 2060 . Ideally, we should have dont stratified sampling because the target variable is binary. Here, the idea is to demonstrate the use of superml for building machine learning models on text data. . Let’s train a random forest model. All machine learning models in superml are known as Trainer. . rf &lt;- RFTrainer$new(max_features = 50, n_estimators = 100) rf$fit(train, &#39;target_var&#39;) preds &lt;- rf$predict(test) print(preds[1:10]) . ## [1] 0 0 0 0 0 0 0 0 0 0 ## Levels: 0 1 . Summary . In this tutorial, we learned to train a random forest model using tfidf ngram features in R. Next, we’ll see how to create a simple ngram bag of words features model in R. .",
            "url": "https://saraswatmks.github.io/blog/r/machinelearning/superml/2020/06/18/tfidf-matrix-superml-R.html",
            "relUrl": "/r/machinelearning/superml/2020/06/18/tfidf-matrix-superml-R.html",
            "date": " • Jun 18, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Practical Tutorial on Asyncio in Python 3.7",
            "content": "Introduction . Asyncio has become quite popular in the python ecosystem. From using it in small functions to large microservices, it’s benefits are widely recognized. . In this blog, I’ll share my understanding of asyncio and how you can see it. Later we’ll focus on gaining hands-on experience in writing asyc code. We’ll try to cover different use-cases that I’ve come across solving different problems. . Table of Contents . What is asyncio? | How asyncio works? | Practical Examples | 1. What is asyncio? . Asyncio is a built-in library in python used to run code concurrently. You might be wondering, “concurrently”? what does that mean. Let’s understand it. . Concurrency is a way of doing multiple tasks but one at a time. For example: Let’s say, you are reading a book. Also, you are checking your whatsapp. You can do both the things, but one at a time. You can’t concentrate on reading the book and check whatsapp at the same time. You would pause one activity in order to do the other. If you are reading the book, you would keep your phone aside. If your whatsapp fires up some notification, you’ll pause reading and check your phone. . Hence, it can be said that you are reading a book and checking your whatsapp “concurrently”. . But, what if you can do tasks simultaneously? Yes, completely possible. That would be called as Parallelism. For example: Talking on phone while driving. You can do both the things together. . 2. How asyncio works ? . Asyncio makes concurrency possible using coroutines. Coroutines are nothing but python functions prefixed with async keyword. For example: . # this is a normal function def numsum(x): return x # this is a coroutine async def num_sum(x): return (x) . The difference between both the functions can be noted by what they return. Let’s see. . print(numsum(10)) . 10 . print(num_sum(10)) . &lt;coroutine object num_sum at 0x10d3ad8c0&gt; /usr/local/Caskroom/miniconda/base/envs/pyenv/lib/python3.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: coroutine &#39;num_sum&#39; was never awaited &quot;&quot;&quot;Entry point for launching an IPython kernel. RuntimeWarning: Enable tracemalloc to get the object allocation traceback . The normal function returned an integer (as expected). But, coroutines return coroutines object which must be awaited using await keyword. async &amp; await for reserved keywords in python. . print(await num_sum(10)) . 10 . If you are running your code in the jupyter notebook, running the above cell will work. But in a script, it wouldn’t work unless the event loop has been initialised. Think of an event loop as current thread responsible to execute the code. Notebooks have default access to event loops, scripts don’t. Hence, assuming you run it in a script, you can do asyncio.run(num_sum(10)): . You might still be wondering, how is asyncio able to switch between the tasks? Let’s see it with the following examples. . def reading_book(): print(&quot;reading page 1&quot;) print(&quot;reading page 2&quot;) print(&quot;reading page 3&quot;) print(&quot;reading page 4&quot;) def checking_whatsapp(): print(&quot;reading new message 1&quot;) print(&quot;reading new message 2&quot;) print(&quot;reading new message 3&quot;) print(&quot;reading new message 4&quot;) . Let’s say we are doing these two tasks reading_book and checking_whatsapp. First, it would be good to see how would our tasks look it if we do it sequentially. . def main(tasks): for task in tasks: task . # execute tasks sequentially main([reading_book(), checking_whatsapp()]) . reading page 1 reading page 2 reading page 3 reading page 4 reading new message 1 reading new message 2 reading new message 3 reading new message 4 . As expected, in sequential mode, we can’t read whatsapp messages without finishing reading the book pages. In other words, reading pages will block us to read message. . Now, what makes a function behaves likes a coroutine internally is a yield command. yield is used to create generators. yield command helps in suspending the current running task while it is running and switches to other task. Let run the above example concurrently. . def reading_book_concur(): print(&quot;reading page 1&quot;) yield print(&quot;reading page 2&quot;) yield print(&quot;reading page 3&quot;) yield print(&quot;reading page 4&quot;) def checking_whatsapp_concur(): print(&quot;reading new message 1&quot;) yield print(&quot;reading new message 2&quot;) yield print(&quot;reading new message 3&quot;) yield print(&quot;reading new message 4&quot;) . def main(tasks): while tasks: current = tasks.pop(0) try: next(current) tasks.append(current) except StopIteration: pass . # execute tasks concurrently main([reading_book_concur(), checking_whatsapp_concur()]) . reading page 1 reading new message 1 reading page 2 reading new message 2 reading page 3 reading new message 3 reading page 4 reading new message 4 . Now you see, we are doing both the tasks concurrently. Don’t worry if you don’t understand the above function, asyncio provides us much simpler way to do it. Let’s see how we can do in one line with asyncio: . import asyncio # convert to coroutine async def reading_book(): print(&quot;reading page 1&quot;) await asyncio.sleep(0) print(&quot;reading page 2&quot;) await asyncio.sleep(0) print(&quot;reading page 3&quot;) await asyncio.sleep(0) print(&quot;reading page 4&quot;) # convert to coroutine async def checking_whatsapp(): print(&quot;reading new message 1&quot;) await asyncio.sleep(0) print(&quot;reading new message 2&quot;) await asyncio.sleep(0) print(&quot;reading new message 3&quot;) await asyncio.sleep(0) print(&quot;reading new message 4&quot;) . async def main(tasks): await asyncio.gather(*[task for task in tasks]) . await main([reading_book(), checking_whatsapp()]) . reading page 1 reading new message 1 reading page 2 reading new message 2 reading page 3 reading new message 3 reading page 4 reading new message 4 . Few points to note: . asyncio.sleep does the same function as yield, suspends the execution of current task so it can do the other task. | asyncio.gather does similar to main function from previous task, switches between tasks. | . Let’s make it more interesting. Let’s say you also want boil some water also while doing other tasks. How can we do that ? Exactly same as above, but since boiling water would take some time, we’ll modify the sleep time, show below: . async def boiling_water(): print(&quot;boiling for 1 sec&quot;) await asyncio.sleep(2) print(&quot;boiling for 3 sec&quot;) await asyncio.sleep(1) print(&quot;boiling for 5 sec&quot;) async def reading_book(): print(&quot;reading page 1&quot;) await asyncio.sleep(0) print(&quot;reading page 2&quot;) await asyncio.sleep(0) print(&quot;reading page 3&quot;) await asyncio.sleep(0) print(&quot;reading page 4&quot;) async def checking_whatsapp(): print(&quot;reading new message 1&quot;) await asyncio.sleep(0) print(&quot;reading new message 2&quot;) await asyncio.sleep(0) print(&quot;reading new message 3&quot;) await asyncio.sleep(0) print(&quot;reading new message 4&quot;) . await main([boiling_water(),reading_book(), checking_whatsapp()]) . boiling for 1 sec reading page 1 reading new message 1 reading page 2 reading new message 2 reading page 3 reading new message 3 reading page 4 reading new message 4 boiling for 3 sec boiling for 5 sec . We kept the water for boiling and meanwhile we continued to do other tasks. . 3. Practical Examples . The best use of asyncio I have come across is when calling multiple external services where each call is independent of other. Besides concurrency, asyncio can be used with other multiprocessing libraries to enable parallelism. So cool right? In the examples below, we’ll use built-in concurrent python module to use async code but in parallel. . 3.1 Calling external service / api / db in parallel . from concurrent.futures import ThreadPoolExecutor import asyncio async def get_async_response(func, param): loop = asyncio.get_running_loop() # &quot;None&quot; will use all cores threads = ThreadPoolExecutor(max_workers=None) # send tasks to each worker blocking_tasks = [loop.run_in_executor(threads, func, x) for x in param] results = await asyncio.gather(*blocking_tasks) results = [x for x in results if x] return results . Say, we have a list containing some data for which we want response from external api. We can do something like: . import requests my_list = [&#39;id1&#39;, &#39;id2&#39;, &#39;id2&#39;] endpointurl = &quot;https://myurl.com/&quot; def get_response(user_id): r = requests.request(&#39;GET&#39;, f&quot;{endpointurl}{use_id}&quot;).json() return r responses = await get_async_response(get_response, my_list) . 3.2 Doing CPU intensive computation . Let’s say we want to do fuzzy match between two list. Since this is cpu intensive task, we won’t use async for this. . # some dummy data list_size = 5 list1 = [&#39;football is famous&#39;, &#39;tajmahal is famous&#39;, &#39;tajmahal in india&#39;]*list_size list2 = [&#39;messi plays football&#39;, &#39;messi is famous&#39;, &#39;india is in aisa&#39;]*list_size . from fuzzywuzzy import fuzz from concurrent.futures import ProcessPoolExecutor def cpu_tasks(func, *args): # set chunksize to be even with ProcessPoolExecutor() as tp: result = tp.map(func, chunksize=10, *args) return list(result) def fuzzy_match(args): q1, q2 = args[0], args[1] return {(q1, q2): fuzz.token_sort_ratio(q1, q2)} . # creating a list of inputs obj_lst = [(i, j,) for i in list1 for j in list2] . # computing fuzzy matches in parallel r = cpu_tasks(fuzzy_match, obj_lst) . r[1::15] . [{(&#39;football is famous&#39;, &#39;messi is famous&#39;): 55}, {(&#39;tajmahal is famous&#39;, &#39;messi is famous&#39;): 67}, {(&#39;tajmahal in india&#39;, &#39;messi is famous&#39;): 19}, {(&#39;football is famous&#39;, &#39;messi is famous&#39;): 55}, {(&#39;tajmahal is famous&#39;, &#39;messi is famous&#39;): 67}, {(&#39;tajmahal in india&#39;, &#39;messi is famous&#39;): 19}, {(&#39;football is famous&#39;, &#39;messi is famous&#39;): 55}, {(&#39;tajmahal is famous&#39;, &#39;messi is famous&#39;): 67}, {(&#39;tajmahal in india&#39;, &#39;messi is famous&#39;): 19}, {(&#39;football is famous&#39;, &#39;messi is famous&#39;): 55}, {(&#39;tajmahal is famous&#39;, &#39;messi is famous&#39;): 67}, {(&#39;tajmahal in india&#39;, &#39;messi is famous&#39;): 19}, {(&#39;football is famous&#39;, &#39;messi is famous&#39;): 55}, {(&#39;tajmahal is famous&#39;, &#39;messi is famous&#39;): 67}, {(&#39;tajmahal in india&#39;, &#39;messi is famous&#39;): 19}] . Summary . In this tutorial, we got a basic understanding of how concurrency works, how asyncio makes things much easier in running code concurrently. Also, we looked at some examples using parallelism effectively. . In my experience, using asyncio doesn’t make sense when you are doing just one task. For example: You create an endpoint which calls a database for some information, without that rest of the code cannot execute. In such scenario, using concurrent code wouldn’t help since the nature of code is sequential. However, in case you are doing multiple independent tasks at a time, async will be helpful. .",
            "url": "https://saraswatmks.github.io/blog/python/machinelearning/2020/05/03/asyncio-concurrent-python-tutorial.html",
            "relUrl": "/python/machinelearning/2020/05/03/asyncio-concurrent-python-tutorial.html",
            "date": " • May 3, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Useful Full Text Search (FTS) queries with sqlite in Python 3.7",
            "content": "Introduction . Sqlite offers a powerful way to build applications enabled with text similarity functionality. Being written in C, it’s execution speed in unparallel. The ease of use and its flexibity is super nice. You could use it to build a powerful search engine in no time. In fact, I’ve also used for a near real time text search task in a microservice. Here are few quick pointer you should remember: . sqlite creates an in-memory database i.e. everything gets saved into RAM. So, make sure your machine has sufficient ram, incase you are working on large datasets. | The straight forward way of using sqlite is using an conda environment (shown below). Otherwise, it’s a bit tricky to enable full text search (FTS) module with sqlite. | Full text search (FTS) module is quite old in sqlite. We’ll use the latest version, FTS5. | . In this tutorial, I’ll provide you some common &amp; useful fts search queries which I’ve often used in my projects. . Table of Contents . FTS Basics | Setup conda environment | Load the dataset | Indexing pandas dataframe into sqlite | Useful text search queries | 1. FTS Basics . Sqlite’s FTS module creates a virtual table. Think of it as a normal table in a database but on steroids i.e. powered with a blazing fast text search capabilities. . Remember the following points while using a virtual table: . You can’t specify the column types (like schema), every column would be mapped as a text (string) column. | It supports standard table operations such as INSERT, DELETE, UPDATE. | The table assigns an implicit rowid ID for each row which is easily accessible (shown below). | The recommended way to do matching is using the MATCH operator. | The default scoring algorithm used is BM25 (Best matching 25) algorithm. The best match gets highest score. The default sorting in sqlite, is sort by ascending. Hence, in order to keep the result consistent, the score is multiplied by -1. This way the best match can be ranked first. Don’t get confused if you see negative scores. | . 2. Setup conda environment . Like mentioned above, the simplest way to use fts5 module is using sqlite3 installation in conda. Let’s create a new conda environment. Go to your terminal and execute the following commands: . &gt;&gt;&gt; conda create -n pyenv python=3.7 &gt;&gt;&gt; conda activate pyenv . Now, simply launch ipython or jupyter notebook inside the environment to access full functionalities of sqlite3. . 3. Load Dataset . For this tutorial, we’ll use the classic imdb data set which contains ratings for each movie. . import sqlite3 import pandas as pd df = pd.read_csv(&#39;imdb_1000.csv&#39;) . Let’s quickly explore the data and understand what we’ve got. . r, c = df.shape print(f&quot;The data has {r} row and {c} columns&quot;) . The data has 979 row and 6 columns . df.sample(5) . star_rating title content_rating genre duration actors_list . 88 8.4 | The Kid | NOT RATED | Comedy | 68 | [u&#39;Charles Chaplin&#39;, u&#39;Edna Purviance&#39;, u&#39;Jack... | . 759 7.6 | Robin Hood | G | Animation | 83 | [u&#39;Brian Bedford&#39;, u&#39;Phil Harris&#39;, u&#39;Roger Mil... | . 572 7.8 | The Birds | PG-13 | Horror | 119 | [u&#39;Rod Taylor&#39;, u&#39;Tippi Hedren&#39;, u&#39;Suzanne Ple... | . 773 7.6 | Disconnect | R | Drama | 115 | [u&#39;Jason Bateman&#39;, u&#39;Jonah Bobo&#39;, u&#39;Haley Ramm&#39;] | . 84 8.4 | Requiem for a Dream | R | Drama | 102 | [u&#39;Ellen Burstyn&#39;, u&#39;Jared Leto&#39;, u&#39;Jennifer C... | . Note: For now, we’ll be using just title,genre and rating field for query matching with basic text cleaning. . 4. Indexing pandas dataframe into sqlite . Before indexing the data, let’s do basic text cleaning. . # clean text df[&#39;title&#39;] = ((df[&#39;title&#39;] .str #replace everything which is not a digit or alphabet .replace(r&#39;[^a-zA-Z0-9]&#39;,&#39; &#39;) .str .split() .apply(lambda x: &#39; &#39;.join([i.strip() for i in x])) #convert to lowercase .str.lower())) df[&#39;genre&#39;] = df[&#39;genre&#39;].str.lower() . # create sqlite database into local memory (RAM) db = sqlite3.connect(&#39;:memory:&#39;) cur = db.cursor() . # create table cur.execute(&#39;create virtual table imdb using fts5(title, genre, rating, tokenize=&quot;porter unicode61&quot;);&#39;) . &lt;sqlite3.Cursor at 0x11e2cd5e0&gt; . sqlite offers powerful inbuilt tokenizer options. Those are: . unicode61: This is the default. It normalises all tokens into unicode characters. | ascii: It converts all non-ascii characters like ã, Â and matches them with their ascii version. For example: porteño would be matched with porteno. | porter: It implements porter stemming algorithm. It would match happening, happened, happens to happen. | . For our case, we are using a combination of unicode61 and porter tokenizer. . # bulk index records cur.executemany(&#39;insert into imdb (title, genre, rating) values (?,?,?);&#39;, df[[&#39;title&#39;, &#39;genre&#39;,&#39;star_rating&#39;]].to_records(index=False)) db.commit() . 5. Useful text search queries . Query 1 . * wildcard is quite powerful here. It can match sub queries (matches god -&gt; godfather) as well. . # match any tokens which begins with this prefix q = &#39;god*&#39; res = cur.execute(f&quot;&quot;&quot;select *, rank from imdb where title MATCH &quot;{q}&quot; and rating &gt; 6 ORDER BY rank limit 5&quot;&quot;&quot;).fetchall() res . [(&#39;the godfather&#39;, &#39;crime&#39;, 9.2, -5.50716086129246), (&#39;city of god&#39;, &#39;crime&#39;, 8.7, -5.126347695889207), (&#39;the godfather part ii&#39;, &#39;crime&#39;, 9.1, -4.794793805845165), (&#39;the godfather part iii&#39;, &#39;crime&#39;, 7.6, -4.794793805845165), (&#39;aguirre the wrath of god&#39;, &#39;adventure&#39;, 8.0, -4.503522057306445)] . Query 2 . # everything starts with this word q = &#39;^ The&#39; res = cur.execute(f&quot;&quot;&quot;select *, rank from imdb where title MATCH &quot;{q}&quot; ORDER BY rank limit 5&quot;&quot;&quot;).fetchall() res . [(&#39;the godfather&#39;, &#39;crime&#39;, 9.2, -1.30318200237765), (&#39;the matrix&#39;, &#39;action&#39;, 8.7, -1.30318200237765), (&#39;the intouchables&#39;, &#39;biography&#39;, 8.6, -1.30318200237765), (&#39;the pianist&#39;, &#39;biography&#39;, 8.5, -1.30318200237765), (&#39;the departed&#39;, &#39;crime&#39;, 8.5, -1.30318200237765)] . Query 3 . Here it matches the titles where exists maximum 3 tokens between the and a token. . # match all title where there are maximum 3 tokens between &quot;the&quot; and &quot;a&quot;. # good way to match phrases res = cur.execute(f&quot;&quot;&quot;select *, rank from imdb where title MATCH &#39;NEAR(the a, 3)&#39; ORDER BY rank limit 5&quot;&quot;&quot;).fetchall() res . [(&#39;the perks of being a wallflower&#39;, &#39;drama&#39;, 8.1, -3.0937328318311303), (&#39;perfume the story of a murderer&#39;, &#39;crime&#39;, 7.5, -3.0937328318311303), (&#39;once upon a time in the west&#39;, &#39;western&#39;, 8.6, -2.9261560330036995)] . Query 4 . Easy to use boolean operators between tokens. AND, OR and NOT are reserved keywords in sqlite. . q = &#39;The OR GodFather&#39; res = cur.execute(f&quot;&quot;&quot;select *, rank from imdb where title MATCH &quot;{q}&quot; ORDER BY rank limit 5&quot;&quot;&quot;).fetchall() res . [(&#39;the godfather&#39;, &#39;crime&#39;, 9.2, -6.81247515893631), (&#39;the godfather part ii&#39;, &#39;crime&#39;, 9.1, -5.931261954617385), (&#39;the godfather part iii&#39;, &#39;crime&#39;, 7.6, -5.931261954617385), (&#39;the lord of the rings the return of the king&#39;, &#39;adventure&#39;, 8.9, -1.0803071105367759), (&#39;the lord of the rings the fellowship of the ring&#39;, &#39;adventure&#39;, 8.8, -1.0803071105367759)] . Query 5 . # hybrid query q = &#39;The AND GodFather AND P*&#39; res = cur.execute(f&quot;&quot;&quot;select *, rank from imdb where title MATCH &quot;{q}&quot; ORDER BY rank limit 5&quot;&quot;&quot;).fetchall() res . [(&#39;the godfather part ii&#39;, &#39;crime&#39;, 9.1, -7.960962698923183), (&#39;the godfather part iii&#39;, &#39;crime&#39;, 7.6, -7.960962698923183)] . Query 6 . # hybrid query q = &#39;a AND the OR a NOT of*&#39; res = cur.execute(f&quot;&quot;&quot;select *, rank from imdb where title MATCH &quot;{q}&quot; ORDER BY rank limit 5&quot;&quot;&quot;).fetchall() res . [(&#39;once upon a time in the west&#39;, &#39;western&#39;, 8.6, -5.2526889873547455), (&#39;a prophet&#39;, &#39;crime&#39;, 7.9, -3.1906709638269364), (&#39;boy a&#39;, &#39;drama&#39;, 7.7, -3.1906709638269364), (&#39;the perks of being a wallflower&#39;, &#39;drama&#39;, 8.1, -3.0937328318311303), (&#39;perfume the story of a murderer&#39;, &#39;crime&#39;, 7.5, -3.0937328318311303)] . Query 7 . # hybrid query q = &#39;com*&#39; res = cur.execute(f&quot;&quot;&quot;select *, rank from imdb where title MATCH &quot;{q}&quot; ORDER BY rank limit 5&quot;&quot;&quot;).fetchall() res . [(&#39;the ten commandments&#39;, &#39;adventure&#39;, 7.9, -5.326063808508073), (&#39;the king of comedy&#39;, &#39;comedy&#39;, 7.8, -4.981592992424006), (&#39;guess who s coming to dinner&#39;, &#39;comedy&#39;, 7.8, -4.411015485745623), (&#39;master and commander the far side of the world&#39;, &#39;action&#39;, 7.4, -3.764289031066617)] . Query 8 . Instead of using the search field in the sql query, we can also mention it in the search query instead. . # hybrid query q = &#39;title : of the&#39; res = cur.execute(f&quot;&quot;&quot;select *, rank from imdb where imdb MATCH &quot;{q}&quot; ORDER BY rank limit 5&quot;&quot;&quot;).fetchall() res . [(&#39;nausicaa of the valley of the wind&#39;, &#39;animation&#39;, 8.2, -3.276218025553669), (&#39;dawn of the planet of the apes&#39;, &#39;action&#39;, 7.7, -3.276218025553669), (&#39;rise of the planet of the apes&#39;, &#39;action&#39;, 7.6, -3.276218025553669), (&#39;the lord of the rings the return of the king&#39;, &#39;adventure&#39;, 8.9, -3.213583634242071), (&#39;the lord of the rings the fellowship of the ring&#39;, &#39;adventure&#39;, 8.8, -3.213583634242071)] . Following queries would demonstrate the use of available auxilary functions in sqlite. . Query 9 . highlight as the name suggest is useful to highlight the selected text using given values. . # highlight text with brackets q = &#39;title : of the&#39; res = cur.execute(f&quot;&quot;&quot;select highlight(imdb, 0, &#39;[&#39;, &#39;]&#39;) from imdb where imdb MATCH &quot;{q}&quot; ORDER BY rank limit 5&quot;&quot;&quot;).fetchall() res . [(&#39;nausicaa [of] [the] valley [of] [the] wind&#39;,), (&#39;dawn [of] [the] planet [of] [the] apes&#39;,), (&#39;rise [of] [the] planet [of] [the] apes&#39;,), (&#39;[the] lord [of] [the] rings [the] return [of] [the] king&#39;,), (&#39;[the] lord [of] [the] rings [the] fellowship [of] [the] ring&#39;,)] . Query 10 . snippet is used to extract the given search query from the text. Below, we extract upto three words around the search query. . # hybrid query q = &#39;title : the&#39; res = cur.execute(f&quot;&quot;&quot;select snippet(imdb, 0, &#39;[&#39;, &#39;]&#39;, &#39;&#39;, 3) from imdb where imdb MATCH &quot;{q}&quot; ORDER BY rank limit 5&quot;&quot;&quot;).fetchall() res . [(&#39;[the] lord of&#39;,), (&#39;[the] lord of&#39;,), (&#39;[the] good [the]&#39;,), (&#39;[the] lord of&#39;,), (&#39;[the] hobbit [the]&#39;,)] . Query 11 . bm25 is also provided as a auxilary function. By default, bm25 provides equal weights to all the fields, however here we have the option to provide custom weight. Here, we provide weight = 10 for title and weight = 5 for genre. . # hybrid query q = &#39;(title : the OR of) AND (genre: Action OR Comedy)&#39; res = cur.execute(f&quot;&quot;&quot;select rowid, *, bm25(imdb, 10.0, 5.0) from imdb where imdb MATCH &quot;{q}&quot; ORDER BY bm25(imdb, 10.0, 5.0) limit 5&quot;&quot;&quot;).fetchall() res . [(581, &#39;the king of comedy&#39;, &#39;comedy&#39;, 7.8, -8.870776402745351), (624, &#39;dawn of the planet of the apes&#39;, &#39;action&#39;, 7.7, -8.68632546114357), (788, &#39;rise of the planet of the apes&#39;, &#39;action&#39;, 7.6, -8.68632546114357), (197, &#39;guardians of the galaxy&#39;, &#39;action&#39;, 8.1, -8.666804134844527), (510, &#39;the last of the mohicans&#39;, &#39;action&#39;, 7.8, -8.624016931126334)] . Summary . In this tutorial, we learnt about the basics of sqlite and how to write powerful fts queries for matching text using python 3.7. .",
            "url": "https://saraswatmks.github.io/blog/python/sqlite/2020/04/30/sqlite-fts-search-queries.html",
            "relUrl": "/python/sqlite/2020/04/30/sqlite-fts-search-queries.html",
            "date": " • Apr 30, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://saraswatmks.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://saraswatmks.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hola . I am a data scientist working for more than five years now. I like solving impactful problems because of the challenges they offer and learning opportunities one can discover along the way. I usually follow a three step framework while solving a problem: Feasability, Prototype and Production. . I’ve been mainly working on projects involving text data using deep learning. Some of the use cases I’ve worked on are: . Search Models: Building models to enhance relevancy in search results | Recommendation Models: Improving user discovery of products on a platform | Text Duplication: Building a data product using open source technologies which can efficiently identify duplicate text. | . Also, I am the author &amp; maintainer of superml R package. It helps people build ML models in R just like using scikit-learn. A lot of my weekend time goes in improving it. . To further know about my latest gigs, checkout my linkedin. .",
          "url": "https://saraswatmks.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://saraswatmks.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}